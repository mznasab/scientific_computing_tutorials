{
 "metadata": {
  "name": "",
  "signature": "sha256:9d56987b57c7b4db85c04ceb9bf1fd2dc5dd02b3206862d307e96aad31ef6931"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src='https://www.rc.colorado.edu/sites/all/themes/research/logo.png'>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Introduction to Spark"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Monte Lunacek"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Landscape of Distributed Computing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"https://s3.amazonaws.com/research_computing_tutorials/MTC.svg\" style=\"height:400px\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How do you process 100's of GB of data?\n",
      "\n",
      "- Filtering unstructured data\n",
      "- Aggregation\n",
      "- Large-scale machine learning\n",
      "- Graph analysis"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Outline"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Functional programming in Python\n",
      "- Hadoop's MapReduce\n",
      "- Spark's programming model\n",
      "- As many examples as we can get through!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Functional Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<blockquote>\n",
      "Python acquired lambda, reduce, filter and map, courtesy of a Lisp hacker who missed them and submitted working patches. -Guido van Rossum\n",
      "</blockquote>\n",
      "\n",
      "- `map` \n",
      "- `reduce`\n",
      "- `filter`\n",
      "- `lambda`\n",
      "- And more: [itertools](https://docs.python.org/2/library/itertools.html), [pytoolz](https://github.com/pytoolz/toolz/)\n",
      "\n",
      "We will use these concepts (and more) in `Spark`"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The `map` abstraction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def square(x):\n",
      "    return x*x\n",
      "\n",
      "numbers = [1,2,3]\n",
      "\n",
      "def map_squares(nums):\n",
      "    res = []\n",
      "    for x in nums:\n",
      "        res.append( square(x) )\n",
      "    return res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "or..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = map(square, numbers)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "For parallel computing in python, `map` is a key abstraction."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from multiprocessing import Pool\n",
      "pool = Pool(5)\n",
      "results = pool.map(square, numbers)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "`lambda`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Anonymous function: a function without a name"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lambda_square = lambda x: x*x\n",
      "map(lambda_square, range(10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "map(lambda x: x*x, range(10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = map(lambda x: x*x, range(10))\n",
      "print res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "`reduce`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Apply a function with **two** arguments cumulatively to the container."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_num(x1, x2):\n",
      "    return x1+x2\n",
      "\n",
      "print reduce(add_num, res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "285\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print reduce(lambda x,y: x+y, res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "285\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "`filter`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Constructs a new list for items where the applied function is `True`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def less_than(x):\n",
      "    return x>10\n",
      "\n",
      "filter(less_than, res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[16, 25, 36, 49, 64, 81]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filter(lambda x: x>10, res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[16, 25, 36, 49, 64, 81]"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Storage"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Hadoop Definitive Guide](http://www.amazon.com/Hadoop-Definitive-Guide-Tom-White/dp/1449311520) by Thom White\n",
      "\n",
      "- Access speeds are not keeping up with capacity\n",
      "- HDFS motivation: e.g 100 MB/s\n",
      "\n",
      "<img src=\"hdfs.png\" style=\"height:300px\">\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "MapReduce"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[MapReduce: Simplified Data Processing on Large Clusters](http://research.google.com/archive/mapreduce.html)\n",
      "\n",
      "Kernel programming model for working with distributed data.\n",
      "\n",
      "<img src=\"https://s3.amazonaws.com/research_computing_tutorials/mapreduce.png\" style=\"height:300px\">\n",
      "\n",
      "- Inefficiencies with MapReduce"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Spark"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"https://s3.amazonaws.com/research_computing_tutorials/spark.png\" style=\"\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Overview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"distributed_overview.png\" style=\"\">\n",
      "\n",
      "- Spark: same code runs locally or on a cluster."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Spark Programming Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Everything starts with a `SparkContext`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import  SparkContext\n",
      "\n",
      "if 'sc' not in globals():\n",
      "    sc = SparkContext(CLUSTER_URL)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This [gist](http://nbviewer.ipython.org/gist/fperez/6384491/00-Setup-IPython-PySpark.ipynb) by Fernando Perez explains how to initialize the `CLUSTER_URL` during the startup of IPython.\n",
      "\n",
      "- local\n",
      "-  URL for a distributed cluster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print CLUSTER_URL"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "local\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}