{
 "metadata": {
  "name": "",
  "signature": "sha256:c57d812da3e3fa4d66c72125a7450203bf529bfbf2980132ec8eb97a4bfeedd5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src='https://www.rc.colorado.edu/sites/all/themes/research/logo.png'>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Introduction to Spark"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Monte Lunacek\n",
      "\n",
      "<img src=\"https://s3.amazonaws.com/research_computing_tutorials/salt.png\" style=\"margin-left: auto; margin-right: auto; margin-bottom:0;height:300px; display:block;\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Landscape of Distributed Computing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"https://s3.amazonaws.com/research_computing_tutorials/MTC.svg\" style=\"height:400px\">\n",
      "\n",
      "<a href=\"http://datasys.cs.iit.edu/publications/2009_PhD-UChicago_dissertation.pdf\">Many-Task Computing: Bridging the Gap between High Throughput Computing and High Performance Computing</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "How do you process 100's of GB of data?\n",
      "\n",
      "- Filtering unstructured data\n",
      "- Aggregation\n",
      "- Large-scale machine learning\n",
      "- Graph analysis"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Outline"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Functional programming in Python\n",
      "- Hadoop's MapReduce\n",
      "- Spark's programming model\n",
      "- As many examples as we can get through!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Functional Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<blockquote>\n",
      "Python acquired lambda, reduce, filter and map, courtesy of a Lisp hacker who missed them and submitted working patches. -Guido van Rossum\n",
      "</blockquote>\n",
      "\n",
      "- `map` \n",
      "- `reduce`\n",
      "- `filter`\n",
      "- `lambda`\n",
      "- And more: [itertools](https://docs.python.org/2/library/itertools.html), [pytoolz](https://github.com/pytoolz/toolz/)\n",
      "\n",
      "We will use these concepts (and more) in `Spark`"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The `map` abstraction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def square(x):\n",
      "    return x*x\n",
      "\n",
      "numbers = [1,2,3]\n",
      "\n",
      "def map_squares(nums):\n",
      "    res = []\n",
      "    for x in nums:\n",
      "        res.append( square(x) )\n",
      "    return res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "or..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = map(square, numbers)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "For parallel computing in python, `map` is a key abstraction."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from multiprocessing import Pool\n",
      "pool = Pool(5)\n",
      "results = pool.map(square, numbers)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "`lambda`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Anonymous function: a function without a name"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lambda_square = lambda x: x*x\n",
      "map(lambda_square, range(10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "map(lambda x: x*x, range(10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = map(lambda x: x*x, range(10))\n",
      "print res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "`reduce`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Apply a function with **two** arguments cumulatively to the container."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_num(x1, x2):\n",
      "    return x1+x2\n",
      "\n",
      "print reduce(add_num, res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "285\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print reduce(lambda x,y: x+y, res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "285\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "`filter`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Constructs a new list for items where the applied function is `True`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def less_than(x):\n",
      "    return x>10\n",
      "\n",
      "filter(less_than, res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[16, 25, 36, 49, 64, 81]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filter(lambda x: x>10, res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[16, 25, 36, 49, 64, 81]"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Data Storage"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Hadoop Definitive Guide](http://www.amazon.com/Hadoop-Definitive-Guide-Tom-White/dp/1449311520) by Thom White\n",
      "\n",
      "- Access speeds are not keeping up with capacity\n",
      "- HDFS motivation: e.g 100 MB/s\n",
      "\n",
      "<img src=\"https://s3.amazonaws.com/research_computing_tutorials/hdfs.png\" style=\"height:300px\">\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "MapReduce"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[MapReduce: Simplified Data Processing on Large Clusters](http://research.google.com/archive/mapreduce.html)\n",
      "\n",
      "Kernel programming model for working with distributed data.\n",
      "\n",
      "<img src=\"https://s3.amazonaws.com/research_computing_tutorials/mapreduce.png\" style=\"height:300px\">\n",
      "\n",
      "- Inefficiencies with MapReduce"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Spark"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"https://s3.amazonaws.com/research_computing_tutorials/spark.png\" style=\"\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Overview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"https://s3.amazonaws.com/research_computing_tutorials/distributed_overview.png\" style=\"\">\n",
      "\n",
      "- Spark: same code runs locally or on a cluster."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Spark Programming Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Everything starts with a `SparkContext`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import  SparkContext\n",
      "\n",
      "if 'sc' not in globals():\n",
      "    sc = SparkContext(CLUSTER_URL,'example')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This [gist](http://nbviewer.ipython.org/gist/fperez/6384491/00-Setup-IPython-PySpark.ipynb) by Fernando Perez explains how to initialize the `CLUSTER_URL` during the startup of IPython.\n",
      "\n",
      "- local\n",
      "-  URL for a distributed cluster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print CLUSTER_URL # could be 'local'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "spark://node1239:7077\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Create RDDs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[RDD Documentation](http://spark.apache.org/docs/0.7.0/api/pyspark/pyspark.rdd.RDD-class.html)\n",
      "\n",
      "The `parallelize` method is a utility for initializing RDDs.\n",
      "\n",
      "- Not efficient (it writes a file and reads back in)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "rdd = sc.parallelize(np.arange(20), numSlices=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Transformations and Actions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Actions** return values\n",
      "\n",
      "- `collect`\n",
      "- `reduce`\n",
      "- `take`\n",
      "- `count`\n",
      "\n",
      "**Transformations** return pointers to new RDDs\n",
      "\n",
      "- `map`, `flatmap`\n",
      "- `reduceByKey`\n",
      "- `filter`\n",
      "- `glom`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What does this look like?\n",
      "\n",
      "- `glom`: Returns an RDD list from each partition of an RDD.\n",
      "- `collect`: Returns a list from all elements of an RDD."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for x in rdd.glom().collect():\n",
      "    print x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 1, 2, 3]\n",
        "[4, 5, 6, 7]\n",
        "[8, 9, 10, 11]\n",
        "[12, 13, 14, 15]\n",
        "[16, 17, 18, 19]\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd = sc.parallelize(np.arange(20), numSlices=10)\n",
      "for x in rdd.glom().collect():\n",
      "    print x"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 1]\n",
        "[2, 3]\n",
        "[4, 5]\n",
        "[6, 7]\n",
        "[8, 9]\n",
        "[10, 11]\n",
        "[12, 13]\n",
        "[14, 15]\n",
        "[16, 17]\n",
        "[18, 19]\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "`map` and `Flatmap`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Return a new RDD by first applying a function and then flattening the results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd = sc.parallelize([ [2, 3, 4],[0, 1],[5, 6, 7, 8] ])\n",
      "rdd.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "[[2, 3, 4], [0, 1], [5, 6, 7, 8]]"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd.map(lambda x: range(len(x))).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "[[0, 1, 2], [0, 1], [0, 1, 2, 3]]"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or I can flatten the results..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd.flatMap(lambda x: range(len(x))).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "[0, 1, 2, 0, 1, 0, 1, 2, 3]"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or flatten the original results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd.flatMap(lambda x: x).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "[2, 3, 4, 0, 1, 5, 6, 7, 8]"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Reduction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd.flatMap(lambda x: x).reduce(lambda x,y: x+y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "36"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
      "rdd.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "[('a', 1), ('b', 1), ('a', 2)]"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd.reduceByKey(lambda x,y: x+y).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "[('b', 1), ('a', 3)]"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd = sc.parallelize([(\"hamlet\", 1), (\"claudius\", 1), (\"hamlet\", 1)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd.countByKey()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "defaultdict(<type 'int'>, {'claudius': 1, 'hamlet': 2})"
       ]
      }
     ],
     "prompt_number": 24
    }
   ],
   "metadata": {}
  }
 ]
}